---
title: "ğŸ§ª PrÃ¡ctica 8 â€” ExperimentaciÃ³n con Redes Neuronales"
date: 2025-09-30
---

# ğŸ§ª PrÃ¡ctica 8 â€” ExperimentaciÃ³n con Redes Neuronales  
**Unidad TemÃ¡tica 2 â€” Deep Learning Foundations**

---

## ğŸ“˜ Contexto General

En esta prÃ¡ctica se profundiza en el comportamiento de las **redes neuronales multicapa (MLP)** a travÃ©s de **experimentaciÃ³n controlada**:  
se modifican los **optimizadores**, **funciones de activaciÃ³n**, **nÃºmero de capas**, y **mÃ©todos de regularizaciÃ³n**, para analizar su efecto sobre el rendimiento y la estabilidad del modelo.

El objetivo no es solo entrenar redes, sino **comprender cÃ³mo cada parÃ¡metro afecta el proceso de aprendizaje** y la capacidad de generalizaciÃ³n.

---

## ğŸ¯ Objetivos

- Comparar el impacto de distintos **optimizadores** (SGD, Adam, RMSprop).  
- Evaluar cÃ³mo cambian los resultados al usar diferentes **activaciones** (`relu`, `tanh`, `sigmoid`).  
- Observar los efectos de **regularizaciÃ³n (dropout, batch normalization)**.  
- Analizar visualmente el **entrenamiento, validaciÃ³n y sobreajuste**.  

---

## âš™ï¸ Paso 1 â€” LibrerÃ­as y ConfiguraciÃ³n Inicial

CÃ³digo base para importar librerÃ­as de **TensorFlow/Keras** y generar datos sintÃ©ticos (`make_moons`).

![moons_data](../assets/po4.png)
---

## ğŸ§© Paso 2 â€” GeneraciÃ³n y Escalado del Dataset

Se generaron 1000 observaciones con dos caracterÃ­sticas.  
`noise=0.2` introduce cierta variabilidad, simulando datos reales con ruido.  
Los valores se escalan para mejorar la **convergencia del entrenamiento**.  


---

## ğŸ§± Paso 3 â€” DefiniciÃ³n del Modelo Base (Sequential)

Se crea una funciÃ³n que construye redes neuronales parametrizables.  
Esto permite reutilizar la misma estructura con diferentes activaciones u optimizadores.

**Estructura:**
- Capa oculta 1 â†’ 16 neuronas.  
- Capa oculta 2 â†’ 8 neuronas.  
- Capa de salida â†’ 1 neurona con activaciÃ³n `sigmoid` (salida binaria).  

---

## ğŸš€ Paso 4 â€” Entrenamiento con Diferentes Optimizadores

Se entrenÃ³ el mismo modelo con tres **optimizadores distintos**: `SGD`, `Adam`, `RMSprop`.  
El objetivo fue observar la rapidez de convergencia y estabilidad de cada uno.

ğŸ“ˆ **VisualizaciÃ³n de resultados:**
![optimizers_plot](../assets/po1.png)

**InterpretaciÃ³n:**
- `Adam` logra la convergencia mÃ¡s rÃ¡pida y estable.  
- `SGD` mejora lentamente pero es mÃ¡s ruidoso.  
- `RMSprop` presenta una curva intermedia entre ambas.  

---

## ğŸ§® Paso 5 â€” EvaluaciÃ³n del DesempeÃ±o

| Optimizador | Exactitud | PÃ©rdida |
|--------------|------------|----------|
| SGD | 0.85 | 0.38 |
| Adam | 0.97 | 0.12 |
| RMSprop | 0.93 | 0.21 |

ğŸ§© El **optimizador Adam** presenta el mejor equilibrio entre rapidez y estabilidad del gradiente.

---

## ğŸ§  Paso 6 â€” ComparaciÃ³n de Funciones de ActivaciÃ³n



**InterpretaciÃ³n:**
- **ReLU** converge rÃ¡pido y evita saturaciÃ³n.  
- **Tanh** suaviza la transiciÃ³n, pero es mÃ¡s lenta.  
- **Sigmoid** puede saturarse (gradiente muy pequeÃ±o).

ğŸ” **ConclusiÃ³n parcial:** ReLU es preferible en capas ocultas; Sigmoid queda reservada para la salida binaria.

---

## ğŸ”„ Paso 7 â€” RegularizaciÃ³n: Dropout y Batch Normalization

Se aÃ±adiÃ³ **BatchNormalization** para estabilizar el rango de activaciones y **Dropout (0.3)** para prevenir sobreajuste.  


**InterpretaciÃ³n:**
- La pÃ©rdida de validaciÃ³n se mantiene estable.  
- Dropout reduce el sobreajuste visible en los primeros modelos.  

---

## ğŸ¯ Paso 8 â€” VisualizaciÃ³n de Frontera de DecisiÃ³n

ğŸ§© **Resultado visual:**  
El modelo regularizado logra **fronteras curvas y suaves**, adaptÃ¡ndose perfectamente a las medias lunas sin sobreajustar.


---

## ğŸ“ˆ Paso 9 â€” Monitoreo de Overfitting

ğŸ“‰ **InterpretaciÃ³n:**
El modelo mantiene una diferencia mÃ­nima entre entrenamiento y validaciÃ³n, lo que indica una **buena generalizaciÃ³n**.

![accuracy_curve](../assets/po2.png)
![monitoreo](../assets/po3.png)
---

## ğŸ§© Resultados Globales

| ConfiguraciÃ³n | Accuracy | Comentario |
|----------------|-----------|-------------|
| PerceptrÃ³n base (SGD) | 0.85 | LÃ­nea recta, pobre separaciÃ³n. |
| MLP con Adam (ReLU) | 0.97 | Convergencia estable y rÃ¡pida. |
| MLP con RMSprop | 0.93 | Buen desempeÃ±o, menor estabilidad. |
| Regularizado (BN + Dropout) | 0.96 | Menor sobreajuste y curvas suaves. |

---

## ğŸ’¬ Conclusiones

1. **Adam + ReLU** fue la combinaciÃ³n mÃ¡s eficiente.  
2. Las funciones **no lineales** y la normalizaciÃ³n son esenciales para lograr buena separaciÃ³n.  
3. **Dropout** y **BatchNormalization** ayudan a evitar el sobreentrenamiento.  
4. Los grÃ¡ficos demuestran que la **curvatura de la frontera** mejora a medida que aumentan las capas ocultas.  
5. El experimento confirma la **importancia del tuning de hiperparÃ¡metros**.

---

## ğŸ¤” Preguntas de ReflexiÃ³n

- Â¿Por quÃ© Adam converge mÃ¡s rÃ¡pido que SGD?  
  â†’ Porque ajusta la tasa de aprendizaje individualmente para cada parÃ¡metro.  

- Â¿QuÃ© pasarÃ­a si usamos demasiadas capas ocultas?  
  â†’ El modelo podrÃ­a sobreajustar y volverse ineficiente computacionalmente.  

- Â¿CuÃ¡l es la funciÃ³n principal de BatchNormalization?  
  â†’ Mantener estables las distribuciones internas, acelerando el entrenamiento.  

- Â¿Por quÃ© ReLU es mejor en capas ocultas que Sigmoid?  
  â†’ ReLU no satura los gradientes positivos y evita el problema del *vanishing gradient*.  

---

## ğŸ“š Evidencias

Guarda las grÃ¡ficas generadas en:  
`docs/assets/practica8/`  
- `moons_dataset.png`  
- `optimizers_comparison.png`  
- `activations_comparison.png`  
- `regularization.png`  
- `decision_boundary.png`  
- `accuracy_curve.png`  

---
ğŸ“ **Evidencias**  


- [![Abrir en Colab](https://colab.research.google.com/drive/1Q0fkH-vNfRu-82r64XpzOumIJ6-lhT0A?usp=sharing) â€” Notebook completo en Google Colab.

---
