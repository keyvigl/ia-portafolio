---
title: "ğŸ§ª PrÃ¡ctica 14 â€” IntroducciÃ³n a LangChain con OpenAI: Prompting, Plantillas y RAG"
date: 2025-10-28
---

# ğŸ§ª PrÃ¡ctica 14 â€” IntroducciÃ³n a LangChain con OpenAI: Prompting, Plantillas y RAG  
**Unidad TemÃ¡tica 4 â€” LLMs, LangChain y OpenAI**

---

## ğŸ“˜ Contexto General

Esta prÃ¡ctica introduce el uso de **LangChain** integrado con **OpenAI**, enfocÃ¡ndose en cinco pilares fundamentales:

1. **Prompting directo** con modelos de lenguaje.  
2. **ChatMessages (system, user, assistant)** y cÃ³mo estructuran conversaciones.  
3. **PromptTemplates** para separar lÃ³gica, estructura y contenido.  
4. **Chains**: flujos encadenados de entrada â†’ modelo â†’ salida.  
5. **Mini-RAG (Retrieval-Augmented Generation)** con documentos externos.

La prÃ¡ctica sigue la misma lÃ­nea pedagÃ³gica de las anteriores:  
**explicaciÃ³n conceptual + cÃ³digo + interpretaciÃ³n del resultado real**, usando el archivo `14_langchain_openai_intro.py` como base.

---

## ğŸ¯ Objetivos

- Comprender el flujo bÃ¡sico de interacciÃ³n con OpenAI vÃ­a LangChain.  
- Crear y usar **prompts estructurados** y **mensajes en formato chat**.  
- Construir **PromptTemplates** reutilizables.  
- Ejecutar **LLMChains** para automatizar interacciÃ³n.  
- Generar **structured outputs** en formato JSON.  
- Implementar un **Mini-RAG**: carga de textos, embeddings y retrieval.  
- Interpretar resultados y entender su impacto en aplicaciones reales.

---

# âš™ï¸ Paso 1 â€” ConfiguraciÃ³n Inicial

El archivo carga:

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.schema import HumanMessage, SystemMessage
```

Se establece la **clave de API** y se inicializa un modelo:

```python
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
```

---

## ğŸ” InterpretaciÃ³n

- `temperature=0.3` genera respuestas **consistentes, poco aleatorias**, ideal para tareas controladas.  
- Usamos `ChatOpenAI` porque permite trabajar con **mensajes** y no solo cadenas de texto.  

---

# ğŸ§© Paso 2 â€” Primer Prompt Simple

CÃ³digo en  archivo:

```python
respuesta = llm.invoke("DefinÃ­ 'Transformer' en una sola oraciÃ³n..")
print(respuesta.content)
```

### ğŸ“„ Resultado esperado:

```
Un Transformer es una arquitectura de redes neuronales que utiliza mecanismos de atenciÃ³n, especialmente la atenciÃ³n sobre toda la secuencia (self-attention), para procesar entradas en paralelo y capturar dependencias a largo plazo sin recurrencia, lo que la hace especialmente eficaz para el procesamiento de lenguaje natural y otras tareas.

```

---

# ğŸ§© Paso 3 â€” Mensajes Tipo Chat

CÃ³digo del archivo:

```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "Sos un asistente conciso, exacto y profesional."),
    ("human", "ExplicÃ¡ {tema} en <= 3 oraciones, con un ejemplo real."),
    ("human", "Por ejemplo, cuando hablamos de 'atenciÃ³n multi-cabeza', se refiere a un mecanismo donde mÃºltiples 'cabezas' o capas de atenciÃ³n permiten que el modelo enfoque diferentes partes de la entrada simultÃ¡neamente, mejorando su capacidad de procesamiento de informaciÃ³n.")
])

chain = prompt | llm

print(chain.invoke({"tema": "atenciÃ³n multi-cabeza"}).content)
```

### Resultado :

```
La atenciÃ³n multi-cabeza realiza varias atenciones paralelas (cabezas) sobre la misma entrada y luego concatena sus salidas para obtener una representaciÃ³n mÃ¡s rica. Cada cabeza puede enfocarse en un aspecto distinto, como relaciones gramaticales, dependencias a larga distancia o contexto local. Ejemplo real: en el Transformer original (Vaswani et al., 2017), mÃºltiples cabezas de atenciÃ³n permiten al modelo alinear diferentes palabras durante la traducciÃ³n (una cabeza vincula sujeto con verbo, otra verbo con objeto), mejorando la coherencia frente a una Ãºnica atenciÃ³n.

```

---

# ğŸ§ InterpretaciÃ³n

- El **SystemMessage** define el rol del modelo â†’ *experto*.  
- El **HumanMessage** es la consulta.  
- Ã“ptimo para agentes, chatbots y flujos conversacionales.

---

# ğŸ§© Paso 4 â€” PromptTemplate

```python
template = PromptTemplate(
    input_variables=["tema"],
    template="Explica {tema} como si fueras profesor de secundaria."
)
chain = LLMChain(llm=llm, prompt=template)
respuesta = chain.invoke({"tema": "los algoritmos"})
```

### Resultado esperado:

```
Un algoritmo es un conjunto de pasos ordenados que permiten resolver un problema...
```

---

# ğŸ§© Paso 5 â€” Structured Output (JSON)

```python
json_prompt = "Devuelve la siguiente informaciÃ³n en formato JSON:\n- resumen\n- dificultad (1 a 5)\n- tema_principal"
mensaje = HumanMessage(json_prompt)
respuesta = llm.invoke([mensaje])
```

### Ejemplo esperado:

```json
{
  "resumen": "El texto explica quÃ© es un algoritmo...",
  "dificultad": 2,
  "tema_principal": "conceptos bÃ¡sicos de computaciÃ³n"
}
```

---

## ğŸ§ InterpretaciÃ³n

- **Structured Output** es esencial para pipelines, APIs y extracciÃ³n.  
- Permite validar formato y disminuir errores.

---

# ğŸ§© Paso 6 â€” Chains con contexto

```python
plantilla = PromptTemplate(
    input_variables=["pregunta", "contexto"],
    template="Contexto: {contexto}\nPregunta: {pregunta}\nRespuesta:"
)
respuesta = chain.invoke({
    "pregunta": "Â¿QuÃ© es Python?",
    "contexto": "Python es un lenguaje de programaciÃ³n popular."
})
```

### Resultado esperado:

```
Python es un lenguaje de programaciÃ³n interpretado y versÃ¡til...
```

---

# ğŸ§ InterpretaciÃ³n

Esto introduce el concepto base del **RAG**:  
**Contexto + Pregunta â†’ Respuesta fundamentada**.

---

# ğŸ“š Paso 7 â€” Mini-RAG

CÃ³digo del archivo:

```python
loader = TextLoader("documento.txt")
docs = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = splitter.split_documents(docs)

embeddings = OpenAIEmbeddings()
db = FAISS.from_documents(chunks, embeddings)

retriever = db.as_retriever()
contexto = retriever.get_relevant_documents("Â¿CuÃ¡l es la idea principal del texto?")
```

### Resultado esperado:

```
El documento trata sobre la importancia del pensamiento algorÃ­tmico...
```

---

# ğŸ” InterpretaciÃ³n

El Mini-RAG permite:

- usar tus propios documentos  
- actualizar conocimiento sin reentrenar  
- entregar respuestas verificables

---

# ğŸ“ˆ Paso 8 â€” AnÃ¡lisis General

### ğŸ”¹ Prompting  
Respuesta inmediata, pero poco estructurada.

### ğŸ”¹ Templates  
Separan lÃ³gica y contenido â†’ profesional y escalable.

### ğŸ”¹ Structured Output  
Ideal para extracciÃ³n y anÃ¡lisis de datos.

### ğŸ”¹ Chains  
Automatizan flujos completos.

### ğŸ”¹ RAG  
Convierte LLMs en sistemas **contextuales** basados en documentos reales.

---

# ğŸ§  Conclusiones

1. LangChain facilita el uso profesional de LLMs mediante plantillas, cadenas y mensajes.  
2. Structured Output mejora la robustez y la integraciÃ³n del sistema.  
3. El Mini-RAG demostrÃ³ cÃ³mo extender la capacidad del modelo con informaciÃ³n externa.  
4. La prÃ¡ctica completa sienta bases para agentes, memorias y pipelines avanzados.

---

# ğŸ¤” Preguntas de ReflexiÃ³n

- Â¿Por quÃ© es importante estructurar la conversaciÃ³n con mensajes System/User?  
- Â¿QuÃ© ventajas tiene RAG sobre prompting simple?  
- Â¿En quÃ© escenarios es obligatorio usar JSON estructurado?  
- Â¿QuÃ© mejoras agregarÃ­as a tu propio sistema RAG?

---

# ğŸ“ Evidencias


- ğŸ““ Notebook ejecutado: [![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1neuic9mh5V_S7mYNaR3bUfqxLZtvBDVR?usp=sharing)


- prompting_basico.png  
- chat_messages.png  
- prompt_template.png  
- structured_output.png  
- chain_contexto.png  
- mini_rag_diagrama.png  

---

# ğŸ“¥ Fin del documento
