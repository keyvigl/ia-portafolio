---
title: "üß™ Trabajo Extra ‚Äî Pr√°ctica 8C: Optimizadores y Hiperpar√°metros en MLPs"
date: 2025-10-14
---

# üß™ Trabajo Extra ‚Äî Pr√°ctica 8C  
**Comparaci√≥n de Optimizadores y Hiperpar√°metros en Redes Neuronales Multicapa (MLP)**

---

## üéØ Objetivo

Analizar c√≥mo distintos **optimizadores y configuraciones de hiperpar√°metros** afectan el rendimiento final de un MLP en el dataset **Fashion-MNIST**.

---

## ‚öôÔ∏è Setup del experimento

| Par√°metro | Valor |
|------------|--------|
| Dataset | Fashion-MNIST |
| Modelo | MLP (2 capas ocultas) |
| √âpocas | 10 |
| Batch size | 128 |
| Divisi√≥n | 90% entrenamiento / 10% validaci√≥n |

---

## üß© Optimizadores evaluados

| Optimizador | Hiperpar√°metros explorados | Descripci√≥n |
|--------------|-----------------------------|--------------|
| **Adam** | `learning_rate = {1e-3, 5e-4}` | R√°pida convergencia y estable. |
| **SGD (momentum)** | `lr = 1e-2, momentum = 0.9` | Variante cl√°sica, sensible al LR. |
| **SGD (Nesterov)** | `nesterov=True` | Predice el pr√≥ximo paso del gradiente. |
| **RMSprop** | `lr=1e-3, rho=0.9` | Mantiene media m√≥vil de gradientes. |
| **AdamW** | `lr=1e-3, weight_decay=1e-4` | Control del decaimiento de pesos (regularizaci√≥n). |

---

## üì¶ Pipeline General

```text
Cargar dataset ‚Üí Normalizar ‚Üí Aplanar ‚Üí MLP ‚Üí Compilar con optimizador ‚Üí Entrenar ‚Üí Evaluar
```

### üß† Arquitectura Base
- **Input:** 784 neuronas (28√ó28)  
- **Hidden 1:** 256 (ReLU) + Dropout(0.3)  
- **Hidden 2:** 128 (ReLU)  
- **Output:** 10 (Softmax)

---

## üìä Resultados comparativos

| Optimizador | Accuracy Test | Comentario |
|--------------|----------------|-------------|
| Adam (1e-3) | 0.883 | Buen rendimiento general |
| Adam (5e-4) | 0.888 | M√°s estable, menor oscilaci√≥n |
| SGD (momentum=0.9) | 0.870 | Lento al inicio pero mejora con las √©pocas |
| SGD (Nesterov) | 0.876 | Convergencia m√°s suave |
| RMSprop | 0.881 | Similar a Adam, aunque menos estable |
| AdamW | 0.889 | Mejor balance entre regularizaci√≥n y precisi√≥n |

üìà *AdamW* result√≥ ligeramente superior, mostrando mejor equilibrio entre precisi√≥n y estabilidad.

---

## üé® Visualizaciones

### üîπ Comparaci√≥n de Accuracy final
![optimizers_barplot](../assets/rt1.png)

Cada barra representa la precisi√≥n final de cada optimizador tras 10 √©pocas.  
**AdamW y Adam (5e-4)** destacan con resultados m√°s consistentes.

---

### üîπ Curvas de validaci√≥n
![optimizers_valacc](../assets/rt2.png)

üìç Observaciones:
- **SGD cl√°sico:** m√°s ruidoso y lento al inicio.  
- **Adam / RMSprop:** curvas suaves, convergencia r√°pida.  
- **AdamW:** combinaci√≥n ideal entre suavidad y regularizaci√≥n.

---

## üß† An√°lisis Detallado

### üî∏ Adam
- Excelente balance entre velocidad y estabilidad.  
- Ajustar `learning_rate` entre 5e-4 y 1e-3 da buenos resultados.  
- Ideal para tareas generales de clasificaci√≥n.

### üî∏ SGD + Momentum / Nesterov
- Convergencia m√°s lenta pero m√°s interpretable.  
- Nesterov a√±ade previsi√≥n del gradiente ‚Üí evita oscilaciones grandes.

### üî∏ RMSprop
- Ideal para secuencias o datos ruidosos.  
- Algo inestable si `rho` no est√° bien calibrado.

### üî∏ AdamW
- Regularizaci√≥n expl√≠cita con *weight decay*.  
- Previene sobreajuste en MLP medianos.  
- Recomendado para datasets de im√°genes o NLP.

---

## üìà Conclusi√≥n comparativa

| Tipo | Optimizador recomendado | Raz√≥n |
|------|--------------------------|--------|
| Rendimiento r√°pido | **Adam (5e-4)** | Balance velocidad‚Äìprecisi√≥n |
| Generalizaci√≥n | **AdamW (1e-4)** | Evita overfitting |
| Entrenamiento estable | **SGD + Nesterov** | Suaviza las oscilaciones |
| Casos ruidosos | **RMSprop** | Ajuste adaptativo del gradiente |

---

## üí¨ Reflexi√≥n Personal

> ‚ÄúLos resultados confirman que no existe un optimizador perfecto, sino uno adecuado para cada contexto.‚Äù

- Adam sigue siendo la mejor elecci√≥n general.  
- AdamW introduce ventajas claras cuando hay riesgo de overfitting.  
- La tasa de aprendizaje es el hiperpar√°metro m√°s cr√≠tico: peque√±os cambios generan grandes diferencias.  
- Las curvas de validaci√≥n visuales son esenciales para interpretar convergencia.

---

## üìö Evidencias y Recursos

- [![Abrir en Colab](https://colab.research.google.com/drive/13vhgCjVJt6IBaHYcCjqoViTTwp6_KLYI?usp=sharing) ‚Äî Notebook completo en Google Colab.


---

## üßæ Datos T√©cnicos

- **Notebook:** `Practica8C_Optimizadores_MLP.ipynb`  
- **Framework:** TensorFlow / Keras  
- **Duraci√≥n:** ~20 min (GPU Colab)  
- **Autor:** Keyvi Alexander Garc√≠a Linares  
- **Curso:** Machine Learning ‚Äî UT2: Deep Learning Foundations  
- **Tipo:** Trabajo Extra ‚Äî Optimizaci√≥n avanzada  

---

