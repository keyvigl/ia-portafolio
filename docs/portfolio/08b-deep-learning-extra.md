---
title: "üß™ Trabajo Extra ‚Äî Pr√°ctica 8B: Experimentaci√≥n de Arquitecturas MLP"
date: 2025-10-14
---

# üß™ Trabajo Extra ‚Äî Pr√°ctica 8B  
**Comparaci√≥n de arquitecturas, activaciones y regularizaci√≥n en Redes Neuronales Multicapa (MLP)**

---

## üéØ Objetivo del Experimento

Explorar c√≥mo la **profundidad, activaci√≥n, inicializaci√≥n y regularizaci√≥n** afectan el desempe√±o de un MLP.  
El experimento ampl√≠a la Pr√°ctica 8, aplicando *variaciones arquitect√≥nicas sistem√°ticas* sobre el dataset **Fashion-MNIST**.

---

## üì¶ Dataset Utilizado

| Dataset | Dimensi√≥n | Clases | Tipo | Dificultad |
|----------|------------|--------|------|-------------|
| üëï **Fashion-MNIST** | 28√ó28 (784 features) | 10 | Imagen en escala de grises | Media |

**Motivo de selecci√≥n:** mantiene la simplicidad de MNIST pero a√±ade variaciones de textura, forma y sombra que desaf√≠an los MLP planos.

---

## ‚öôÔ∏è Pipeline General

```text
Cargar dataset ‚Üí Normalizar im√°genes ‚Üí Aplanar ‚Üí Construir MLP ‚Üí Entrenar ‚Üí Evaluar
```

### üîß Configuraci√≥n base

| Par√°metro | Valor |
|------------|--------|
| Optimizador | Adam |
| P√©rdida | SparseCategoricalCrossentropy |
| √âpocas | 10 |
| Batch size | 128 |
| Validaci√≥n | 10% del train |

---

## üß© Arquitecturas Probadas

| Modelo | Capas / Neuronas | Activaci√≥n | Regularizaci√≥n | Inicializaci√≥n | Comentario |
|---------|------------------|-------------|----------------|----------------|-------------|
| **Base ReLU** | 2√ó128 | ReLU | ‚Äî | HeNormal | Baseline |
| **Profundo 4√ó512** | 4√ó512 | ReLU | ‚Äî | HeNormal | Alta capacidad, riesgo de overfitting |
| **Tanh activaci√≥n** | 2√ó128 | Tanh | ‚Äî | HeNormal | Saturaci√≥n en extremos |
| **GELU activaci√≥n** | 2√ó128 | GELU | ‚Äî | HeNormal | Transiciones suaves, moderna |
| **BatchNorm + Dropout** | 2√ó128 | ReLU | Dropout(0.3) + BN | HeNormal | Estabiliza entrenamiento |
| **Regularizaci√≥n L2** | 2√ó128 | ReLU | L2=1e-4 | HeNormal | Penaliza pesos grandes |
| **Inicializador Glorot** | 2√ó128 | ReLU | ‚Äî | GlorotUniform | Equilibrio entre capas |

---

## üìä Resultados de Evaluaci√≥n

| Modelo | Accuracy (Test) | Observaci√≥n |
|---------|----------------|--------------|
| Base ReLU | 0.875 | Buen rendimiento general |
| Profundo 4√ó512 | 0.887 | Ligero overfitting |
| Tanh activaci√≥n | 0.864 | Menor estabilidad, m√°s lenta convergencia |
| GELU activaci√≥n | 0.882 | Aprendizaje m√°s suave, estable |
| BatchNorm + Dropout | 0.891 | Mejor equilibrio entre train/test |
| Regularizaci√≥n L2 | 0.880 | Curvas suaves y menos sobreajuste |
| Inicializador Glorot | 0.878 | Similar a HeNormal |

üìà *La variaci√≥n en accuracy global se mantiene entre 86‚Äì89%, lo que demuestra que los MLP son robustos, pero sensibles a peque√±as configuraciones.*

---

## üé® Visualizaciones del Entrenamiento

### üîπ Comparaci√≥n de Accuracy en Test
![architecture_barplot](../assets/er1.png)

Cada barra representa la arquitectura final tras 10 √©pocas.  
Las mejores combinaciones fueron **BatchNorm + Dropout** y **Profundo 4√ó512**.

---

### üîπ Evoluci√≥n de Precisi√≥n en Validaci√≥n
![val_acc_plot](../assets/er2.png)

- ReLU converge m√°s r√°pido.  
- GELU es m√°s estable pero tarda m√°s en alcanzar su m√°ximo.  
- Tanh presenta oscilaciones t√≠picas por saturaci√≥n.

---

## üß† An√°lisis Comparativo

### üî∏ Activaciones
- **ReLU:** r√°pida convergencia, estable y eficiente (referencia est√°ndar).  
- **Tanh:** m√°s lenta, √∫til cuando se requiere centrado en [-1,1].  
- **GELU:** moderna, combina ventajas de ReLU y Tanh, ideal para arquitecturas profundas.

### üî∏ Regularizaci√≥n
- **Dropout (0.3):** evita memorizar muestras; mejora validaci√≥n.  
- **L2:** √∫til cuando los pesos crecen demasiado; suaviza oscilaciones.  
- **BatchNormalization:** acelera aprendizaje y estabiliza la escala interna.

### üî∏ Inicializaci√≥n
- **HeNormal:** mejor para ReLU (ajusta varianza seg√∫n entrada).  
- **GlorotUniform:** equilibra magnitudes para activaciones sim√©tricas.

---

## üìà Resumen de Rendimiento

| T√©cnica | Beneficio principal | Situaci√≥n ideal |
|----------|---------------------|------------------|
| BatchNorm + Dropout | Generalizaci√≥n y estabilidad | Datos medianos o ruidosos |
| GELU | Suaviza el gradiente | Modelos profundos o sensibles |
| L2 Regularization | Control de pesos grandes | Modelos densos sin Dropout |
| HeNormal Init | Convergencia m√°s r√°pida | Capas ReLU o GELU |
| Glorot Init | Distribuci√≥n balanceada | Capas Tanh |

---

## üí¨ Conclusiones Finales

1. **La profundidad no garantiza mejor rendimiento** sin regularizaci√≥n.  
2. **BatchNormalization + Dropout** result√≥ ser la combinaci√≥n m√°s eficiente y estable.  
3. **GELU** mostr√≥ un aprendizaje m√°s suave que **ReLU**, con mejor estabilidad a largo plazo.  
4. La **regularizaci√≥n L2** mantiene las curvas suaves, ideal para evitar oscilaciones.  
5. Inicializadores como **HeNormal** y **GlorotUniform** ofrecen una base s√≥lida para entrenamientos consistentes.

---

## ü§î Reflexi√≥n Personal

> ‚ÄúEl experimento confirm√≥ que el verdadero poder del deep learning no est√° solo en tener muchas capas, sino en **c√≥mo se configuran y regularizan**.‚Äù

- Aprend√≠ que cada hiperpar√°metro tiene un rol equilibrante.  
- Las visualizaciones me ayudaron a interpretar estabilidad y convergencia.  
- Comprend√≠ que el rendimiento √≥ptimo es resultado de *interacciones sutiles*, no solo fuerza bruta.

---

## üìö Evidencias y Recursos

- [![Abrir en Colab](https://colab.research.google.com/drive/1s5RDVnkRM_sKdiET-dQLClEYfFIJhtVL?usp=sharing) ‚Äî Notebook completo en Google Colab.


```
architecture_barplot.png  
val_acc_plot.png  
```

Referencias oficiales:
- Dense Layers ‚Üí [TensorFlow Docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)  
- Activations ‚Üí [TF Activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations)  
- BatchNormalization ‚Üí [TF Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)  
- Dropout ‚Üí [TF Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)  
- Regularizers ‚Üí [TF Regularizers](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/l2)  
- Initializers ‚Üí [TF Initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)  

---

## üßæ Datos T√©cnicos

- **Notebook:** `Practica8B_Arquitecturas_MLP.ipynb`  
- **Lenguaje:** Python 3.10 + TensorFlow/Keras  
- **Duraci√≥n:** ~15 min (GPU Colab)  
- **Autor:** Keyvi Alexander Garc√≠a Linares  
- **Curso:** Machine Learning ‚Äî UT2: Deep Learning Foundations  
- **Tipo:** Trabajo Extra ‚Äî Experimentaci√≥n Avanzada  

---

üìÅ **Ubicaci√≥n sugerida:**  
`docs/portfolio/08b-Arquitecturas-MLP.md`
