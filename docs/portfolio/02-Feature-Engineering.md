
---
title: "PrÃ¡ctica 2: Feature Engineering simple + Modelo base"
date: 2025-01-01
---

# âš™ï¸ PrÃ¡ctica 2: Feature Engineering simple + Modelo base

## Contexto
En esta prÃ¡ctica trabajamos con el dataset del Titanic para aplicar un **proceso de Feature Engineering simple** y entrenar un **modelo base de clasificaciÃ³n**.  
Se busca entender cÃ³mo transformar variables, crear nuevas caracterÃ­sticas y evaluar un modelo inicial.

## Objetivos
- Practicar la creaciÃ³n de nuevas variables (*features*) a partir de los datos originales.  
- Construir un modelo base de clasificaciÃ³n (Logistic Regression).  
- Comparar su desempeÃ±o con un modelo trivial (*DummyClassifier*).  
- Evaluar mÃ©tricas de rendimiento mÃ¡s allÃ¡ de la accuracy.  
## Actividades (con tiempos estimados)
| Actividad                     | Tiempo |
|--------------------------------|:------:|
| RevisiÃ³n del dataset y limpieza | 20 min |
| CreaciÃ³n de nuevas features     | 30 min |
| Entrenamiento del modelo base   | 25 min |
| EvaluaciÃ³n de mÃ©tricas          | 20 min |


## Desarrollo
## ğŸ” LogisticRegression

### â“ Â¿QuÃ© tipo de problema resuelve?
Los problemas de **clasificaciÃ³n**, de manera mÃ¡s especÃ­fica la **clasificaciÃ³n binaria**.

---

### âš™ï¸ Â¿QuÃ© parÃ¡metros importantes tiene?
- **penalty** â†’ tipo de regularizaciÃ³n.  
- **solver** â†’ algoritmo de optimizaciÃ³n para estimar coeficientes.  
- **max_iter** â†’ nÃºmero mÃ¡ximo de iteraciones del solver.  

---

### ğŸ—ï¸ Â¿CuÃ¡ndo usar `solver='liblinear'` vs otros solvers?
- Principalmente se puede usar para **problemas pequeÃ±os o medianos**.  
- Permite usar penalizaciÃ³n **L1**, dando modelos mÃ¡s dispersos (mÃ¡s coeficientes en cero).  
- Otros solvers como **lbfgs** o **newton-cg** solo soportan penalizaciÃ³n L2 o incluso ninguna.  


## ğŸ”€ train_test_split

### â“ Â¿QuÃ© hace el parÃ¡metro `stratify`?
Asegura que la proporciÃ³n de clases en los conjuntos de entrenamiento y prueba sea la misma que en el dataset completo.

---

### ğŸ² Â¿Por quÃ© usar `random_state`?
- Poder obtener los mismos resultados y compartirlos o depurar con consistencia.  
- Evaluar diferentes modelos o parÃ¡metros sobre la misma divisiÃ³n de datos.  

---

### ğŸ“Š Â¿QuÃ© porcentaje de test es recomendable?
La proporciÃ³n mÃ¡s comÃºnmente recomendada es usar entre **20 % y 30 %** del total para testing  
(lo que implica usar entre **70 % y 80 %** para entrenamiento).  

## ğŸ“ MÃ©tricas de evaluaciÃ³n

### â“ Â¿QuÃ© significa cada mÃ©trica en `classification_report`?
- **Precision**: mide la proporciÃ³n de predicciones positivas que fueron correctas.  
- **Recall**: indica quÃ© fracciÃ³n de instancias positivas reales fueron correctamente identificadas.  
- **F1-score**: la media entre precision y recall, ofreciendo un balance entre ambos.  

---

### ğŸ” Â¿CÃ³mo interpretar la matriz de confusiÃ³n?
- **Filas** â†’ clases reales.  
- **Columnas** â†’ clases predichas.  

---

### âš–ï¸ Â¿CuÃ¡ndo usar accuracy vs otras mÃ©tricas?
- **Accuracy** es intuitiva pero a veces engaÃ±osa.  
- Cuando las clases estÃ¡n **desbalanceadas**, es mejor usar **precisiÃ³n, recall, F1** o mÃ©tricas mÃ¡s sofisticadas que consideren costos distintos o reemplacen el desequilibrio.  


### ğŸ“‚ ConfiguraciÃ³n de rutas en Google Colab

```python
from pathlib import Path
try:
    from google.colab import drive
    drive.mount('/content/drive')
    ROOT = Path('/content/drive/MyDrive/IA-UT1')
except Exception:
    ROOT = Path.cwd() / 'IA-UT1'

DATA_DIR = ROOT / 'data'
RESULTS_DIR = ROOT / 'results'
for d in (DATA_DIR, RESULTS_DIR):
    d.mkdir(parents=True, exist_ok=True)
print('Outputs â†’', ROOT)
```

#### Salida:
```text
Mounted at /content/drive
Outputs â†’ /content/drive/MyDrive/IA-UT1
```
### ğŸ“¥ Descarga del dataset Titanic con Kaggle API

```python
!pip -q install kaggle
from google.colab import files
files.upload()  # SubÃ­ tu archivo kaggle.json descargado
!mkdir -p ~/.kaggle && cp kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json
!kaggle competitions download -c titanic -p data
!unzip -o data/titanic.zip -d data

train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')
```
#### Salida:
```text
Se eligiÃ³ un archivo
Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable.
Saving kaggle.json to kaggle.json
Downloading titanic.zip to data
  0% 0.00/34.1k [00:00<?, ?B/s]
100% 34.1k/34.1k [00:00<00:00, 165MB/s]
Archive:  data/titanic.zip
  inflating: data/gender_submission.csv  
  inflating: data/test.csv           
  inflating: data/train.csv

```
### ğŸ› ï¸ Feature Engineering y preparaciÃ³n de datos

```python
df = train.copy()

# ğŸš« PASO 1: Manejar valores faltantes (imputaciÃ³n)
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])  # Valor mÃ¡s comÃºn
df['Fare'] = df['Fare'].fillna(df['Fare'].median())              # Mediana
df['Age'] = df['Age'].fillna(df.groupby(['Sex','Pclass'])['Age'].transform('median'))

# ğŸ†• PASO 2: Crear nuevas features Ãºtiles
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
df['IsAlone'] = (df['FamilySize'] == 1).astype(int)

df['Title'] = df['Name'].str.extract(',\s*([^\.]+)\.')
rare_titles = df['Title'].value_counts()[df['Title'].value_counts() < 10].index
df['Title'] = df['Title'].replace(rare_titles, 'Rare')

# ğŸ”„ PASO 3: Preparar datos para el modelo
features = ['Pclass','Sex','Age','Fare','Embarked','FamilySize','IsAlone','Title','SibSp','Parch']
X = pd.get_dummies(df[features], drop_first=True)
y = df['Survived']

X.shape, y.shape
```
#### Salida:
```text
((891, 14), (891,))
```
#### ğŸ“Œ InterpretaciÃ³n:
- El dataset final tiene **891 observaciones** (pasajeros).  
- DespuÃ©s del *feature engineering* y la codificaciÃ³n con *dummies*, se generaron **14 columnas predictoras** listas para usar en el modelo.  
- La variable objetivo **`y`** contiene las etiquetas de supervivencia para los mismos 891 pasajeros.  
- âœ… Esto confirma que no se perdieron filas tras la imputaciÃ³n y transformaciÃ³n de variables.  

### ğŸ¤– Entrenamiento y evaluaciÃ³n de modelos

```python
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

dummy = DummyClassifier(strategy='most_frequent', random_state=42)
dummy.fit(X_train, y_train)
baseline_pred = dummy.predict(X_test)

lr = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)
lr.fit(X_train, y_train)
pred = lr.predict(X_test)

print('Baseline acc:', accuracy_score(y_test, baseline_pred))
print('LogReg acc  :', accuracy_score(y_test, pred))

print('\nClassification report (LogReg):')
print(classification_report(y_test, pred))

print('\nConfusion matrix (LogReg):')
print(confusion_matrix(y_test, pred))
```

#### Salida:
```text
Baseline acc: 0.6145251396648045
LogReg acc  : 0.8156424581005587

Classification report (LogReg):
              precision    recall  f1-score   support

           0       0.82      0.89      0.86       110
           1       0.80      0.70      0.74        69

    accuracy                           0.82       179
   macro avg       0.81      0.79      0.80       179
weighted avg       0.81      0.82      0.81       179


Confusion matrix (LogReg):
[[98 12]
 [21 48]]

```
#### ğŸ“Œ InterpretaciÃ³n:
- El modelo base (DummyClassifier) obtiene un accuracy de aproximadamente 61 %, equivalente a predecir siempre la clase mayoritaria (no sobreviviÃ³).  
- La RegresiÃ³n LogÃ­stica mejora el rendimiento con un accuracy de aproximadamente 81.5 %, lo que representa una ganancia importante frente al baseline.  
- SegÃºn la matriz de confusiÃ³n:  
  - Se acierta en la mayorÃ­a de los casos de pasajeros que no sobrevivieron (98).  
  - Se identifican correctamente 48 casos de pasajeros que sÃ­ sobrevivieron.  
  - Se producen 21 falsos negativos (personas que sobrevivieron pero fueron clasificadas como no sobrevivientes).  
  - Se producen 12 falsos positivos (personas que no sobrevivieron pero fueron clasificadas como sobrevivientes).  
- En las mÃ©tricas, la clase 0 (no sobreviviÃ³) alcanza un recall alto (0.89), mientras que la clase 1 (sÃ­ sobreviviÃ³) tiene un recall menor (0.70), lo que indica que el modelo falla mÃ¡s en identificar sobrevivientes.  
- En conclusiÃ³n, la RegresiÃ³n LogÃ­stica supera ampliamente al modelo base, aunque aÃºn presenta limitaciones en la detecciÃ³n de casos positivos de supervivencia.  



### â“ Preguntas para el equipo

**Matriz de confusiÃ³n: Â¿En quÃ© casos se equivoca mÃ¡s el modelo: cuando predice que una persona sobreviviÃ³ y no lo hizo, o al revÃ©s?**  
El modelo se equivoca mÃ¡s al predecir que alguien no sobreviviÃ³ cuando en realidad sÃ­ sobreviviÃ³ (21 casos) que al revÃ©s (12 casos).

---

**Clases atendidas: Â¿El modelo acierta mÃ¡s con los que sobrevivieron o con los que no sobrevivieron?**  
- No sobrevivieron (0) â†’ 98/110 â‰ˆ 89 % de aciertos  
- Sobrevivieron (1) â†’ 48/69 â‰ˆ 70 % de aciertos  
El modelo acierta mÃ¡s con las personas que no sobrevivieron.

---

**ComparaciÃ³n con baseline: Â¿La RegresiÃ³n LogÃ­stica obtiene mÃ¡s aciertos que el modelo que siempre predice la clase mÃ¡s comÃºn?**  
La RegresiÃ³n LogÃ­stica mejora significativamente sobre el baseline, confirmando que el modelo estÃ¡ aprendiendo patrones Ãºtiles.

---

**Errores mÃ¡s importantes: Â¿CuÃ¡l de los dos tipos de error creÃ©s que es mÃ¡s grave para este problema?**  
El error mÃ¡s crÃ­tico son los **falsos negativos (FN)** â†’ predecir que alguien no sobrevivirÃ¡ cuando sÃ­ lo hizo.  
En un escenario real (simulaciÃ³n de rescate), este error serÃ­a mÃ¡s grave porque significa no salvar a alguien que podrÃ­a sobrevivir.

---

**Observaciones generales: Mirando las grÃ¡ficas y nÃºmeros, Â¿quÃ© patrones interesantes encontraste sobre la supervivencia?**  
- Sexo: las mujeres tenÃ­an mucha mÃ¡s probabilidad de sobrevivir que los hombres.  
- Clase: los pasajeros de primera clase tuvieron mayor supervivencia, los de tercera clase menor.  
- Edad: los niÃ±os tenÃ­an mÃ¡s probabilidad de sobrevivir.

---

**Mejoras simples: Â¿QuÃ© nueva columna (feature) se te ocurre que podrÃ­a ayudar a que el modelo acierte mÃ¡s?**  
- InteracciÃ³n entre `Sex` y `Pclass`: algunas clases de mujeres tenÃ­an mÃ¡s prioridad de rescate.  
- Binning de edad: categorizar en niÃ±os, jÃ³venes, adultos y ancianos para capturar patrones no lineales.  
- Presencia de `Cabin`: si la persona tenÃ­a nÃºmero de camarote, podrÃ­a indicar clase alta y, por lo tanto, mayor supervivencia.  


## ğŸ“‚ Evidencias
- Capturas de pantallas del proceso y grÃ¡ficos guardados en `docs/assets/`.  
- Notebook en Google Colab con el desarrollo completo de la prÃ¡ctica:  
[![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ut5NvjzklgNwS8wfOD07xslXUY7flhu4?usp=sharing)  


---

## ğŸ¤” ReflexiÃ³n
- **Lo mÃ¡s desafiante:** manejar los valores faltantes y diseÃ±ar nuevas variables Ãºtiles que aportaran al modelo.  
- **Lo mÃ¡s valioso:** comprobar cÃ³mo el *feature engineering* mejora significativamente el rendimiento respecto al modelo base.  
- **PrÃ³ximos pasos:** probar interacciones entre variables (ej. `Sex Ã— Pclass`), aplicar *binning* a la edad y evaluar modelos mÃ¡s complejos.  

