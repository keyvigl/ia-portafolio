---
title: "Tarea: Validaci√≥n y Selecci√≥n de Modelos - Fill in the Blanks"
date: 2025-01-01
---

# Tarea: Validaci√≥n y Selecci√≥n de Modelos - Fill in the Blanks



## Contexto
En esta tarea se trabaja con **t√©cnicas de validaci√≥n y selecci√≥n de modelos** dentro del flujo de *Machine Learning*.  
El objetivo es aplicar m√©todos como **train/test split, validaci√≥n cruzada y selecci√≥n de hiperpar√°metros** para evaluar de manera justa distintos algoritmos y escoger el mejor en funci√≥n de m√©tricas de desempe√±o.  
Tambi√©n se busca comprender c√≥mo evitar el **sobreajuste (overfitting)** y garantizar que el modelo generalice correctamente a nuevos datos.

## Objetivos
- Aprender a prevenir data leakage usando pipelines
- Implementar validaci√≥n cruzada (cross-validation) robusta
- Comparar m√∫ltiples modelos de forma sistem√°tica
- Interpretar m√©tricas de estabilidad y selecci√≥n de modelos

## Actividades (con tiempos estimados)
| Actividad                                      | Tiempo estimado |
|------------------------------------------------|:---------------:|
| Repaso te√≥rico de validaci√≥n y data leakage    | 30 min |
| Implementaci√≥n de *pipelines* con scikit-learn | 45 min |
| Validaci√≥n cruzada con diferentes modelos      | 45 min |
| Comparaci√≥n de resultados y selecci√≥n final    | 30 min |
| Redacci√≥n de reflexi√≥n y documentaci√≥n final   | 20 min |

## Desarrollo
### üîß Paso 1: Setup Inicial

```python
!pip install ucimlrepo
```
#### Salida:
```text
Collecting ucimlrepo
  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)
Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2.2.2)
Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2025.8.3)
Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)
Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)
Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)
Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)
Installing collected packages: ucimlrepo
Successfully installed ucimlrepo-0.0.7
```
---
```python
# Importar librer√≠as que vamos a usar
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Para validaci√≥n y selecci√≥n de modelos
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
# Para cargar datos desde UCI ML Repository
from ucimlrepo import fetch_ucirepo
from sklearn.metrics import accuracy_score, classification_report
```
#### Salida:
```text
Setup completo!
```
### üéì Paso 2: Cargar y Explorar Datos de Estudiantes
### üìñ Respuestas a las preguntas de investigaci√≥n

**1. ¬øCu√°ntas muestras y caracter√≠sticas tiene el dataset?**  
- El dataset contiene **4,424 estudiantes** y **36 caracter√≠sticas** (features) que incluyen datos acad√©micos, demogr√°ficos y socioecon√≥micos.  

---

**2. ¬øQu√© tipos de variables incluye?**  
- **Demogr√°ficas:** edad al ingresar, estado civil, ocupaci√≥n y nivel educativo de los padres.  
- **Acad√©micas:** modo de aplicaci√≥n, curso, calificaciones en asignaturas, asistencia diurna/nocturna.  
- **Socioecon√≥micas:** becas, ayudas sociales, estatus laboral, entre otras.  

---

**3. ¬øLas clases est√°n balanceadas o desbalanceadas?**  
- La variable objetivo (Target) est√° **moderadamente desbalanceada**:  
  - **Dropout (abandono):** 32.1%  
  - **Graduate (graduado):** 41.8%  
  - **Enrolled (matriculado a√∫n):** 26.1%  
- Aunque no hay una clase minoritaria extrema, es importante tener en cuenta este desbalance para la evaluaci√≥n de modelos.  

---

**4. ¬øQu√© significan las 3 categor√≠as objetivo?**  
- **Dropout:** el estudiante abandon√≥ los estudios universitarios antes de terminar.  
- **Enrolled:** el estudiante sigue matriculado en la universidad.  
- **Graduate:** el estudiante complet√≥ con √©xito la carrera y se gradu√≥.  

```python
# Cargar dataset de estudiantes desde UCI
student_data = fetch_ucirepo(id=697)

# Preparar datos
X = student_data.data.features
y = student_data.data.targets

print("Dataset: Student Dropout and Academic Success")
print(f"Estudiantes: {X.shape[0]}, Caracter√≠sticas: {X.shape[1]}")
print(f"Objetivo: Predecir {len(y.columns)} variable(s)")

# Explorar variable objetivo
target_col = y.columns[0]  # Primera columna objetivo
y_series = y[target_col]
print(f"\nVariable objetivo: {target_col}")

# Mapear valores para mejor interpretaci√≥n
target_mapping = {0: 'Dropout', 1: 'Enrolled', 2: 'Graduate'}
y_mapped = y_series.map(target_mapping)

# Distribuci√≥n de clases
print("\nDistribuci√≥n de resultados acad√©micos:")
value_counts = y_mapped.value_counts()
for outcome, count in value_counts.items():
    percentage = (count / len(y_mapped)) * 100
    print(f"  {outcome}: {count} estudiantes ({percentage:.1f}%)")

# Ver algunas caracter√≠sticas
print(f"\nPrimeras caracter√≠sticas:")
print(X.columns.tolist()[:10], "...")

# Estad√≠sticas b√°sicas
print(f"\nAge at enrollment:")
if 'Age at enrollment' in X.columns:
    age_col = X['Age at enrollment']
    print(f"  Promedio: {age_col.mean():.1f} a√±os")
    print(f"  Rango: {age_col.min():.0f}-{age_col.max():.0f} a√±os")
```

#### Salida:
```text
Dataset: Student Dropout and Academic Success
Estudiantes: 4424, Caracter√≠sticas: 36
Objetivo: Predecir 1 variable(s)

Variable objetivo: Target

Distribuci√≥n de resultados acad√©micos:

Primeras caracter√≠sticas:
['Marital Status', 'Application mode', 'Application order', 'Course', 'Daytime/evening attendance', 'Previous qualification', 'Previous qualification (grade)', 'Nacionality', "Mother's qualification", "Father's qualification"] ...

Age at enrollment:
  Promedio: 23.3 a√±os
  Rango: 17-70 a√±os
```
 üìå Interpretaci√≥n:

- El dataset **Student Dropout and Academic Success** contiene **4,424 estudiantes** y **36 caracter√≠sticas** que abarcan factores **demogr√°ficos, acad√©micos y socioecon√≥micos**.  
- La **variable objetivo (`Target`)** busca clasificar a los estudiantes en tres categor√≠as:  
  - **Dropout** ‚Üí estudiantes que abandonaron.  
  - **Enrolled** ‚Üí estudiantes que contin√∫an matriculados.  
  - **Graduate** ‚Üí estudiantes que lograron graduarse.  
- Las **primeras caracter√≠sticas** incluyen variables relevantes como estado civil, modo de aplicaci√≥n, orden de aplicaci√≥n, curso, calificaciones previas, nacionalidad y nivel educativo de los padres.  
- En cuanto a la edad, el promedio de ingreso es de **23.3 a√±os**, con un rango que va desde los **17 hasta los 70 a√±os**, mostrando una **alta diversidad en la poblaci√≥n estudiantil**.  

Este an√°lisis inicial confirma que el dataset es **rico en variables heterog√©neas** y plantea un **problema de clasificaci√≥n multiclase** que requerir√° manejar posibles desbalances y considerar tanto m√©tricas de desempe√±o como la interpretaci√≥n de factores de riesgo estudiantil.





### üî¨ Parte 1: Cross-Validation - Validaci√≥n Robusta
#### üîß Paso 3: Preparar datos para validaci√≥n
```python
# Preparar variable objetivo como serie simple
# Convertir strings a n√∫meros para sklearn
target_mapping = {0: 'Dropout', 1: 'Enrolled', 2: 'Graduate'}
reverse_mapping = {'Dropout': 0, 'Enrolled': 1, 'Graduate': 2}

# Si y_series contiene strings, convertir a n√∫meros
if y_series.dtype == 'object':
    y_target = y_series.map(reverse_mapping)
else:
    y_target = y_series

X_features = X       # Features del dataset

print("Datos preparados para validaci√≥n:")
print(f"X shape: {X_features.shape}")
print(f"y shape: {y_target.shape}")
print(f"Clases √∫nicas: {sorted(y_target.unique())}")
print(f"Mapeo: {target_mapping}")
```
#### Salida:
```text
Datos preparados para validaci√≥n:
X shape: (4424, 36)
y shape: (4424,)
Clases √∫nicas: [np.int64(0), np.int64(1), np.int64(2)]
Mapeo: {0: 'Dropout', 1: 'Enrolled', 2: 'Graduate'}
```
 üìå Interpretaci√≥n:

- Se prepararon los datos para el proceso de **validaci√≥n de modelos**.  
- La matriz de caracter√≠sticas **X** contiene **4,424 registros** con **36 variables explicativas**.  
- La variable objetivo **y** fue convertida a valores num√©ricos (`0`, `1`, `2`) para ser compatible con *scikit-learn*.  
- Las clases √∫nicas corresponden al mapeo:  
  - `0 ‚Üí Dropout`  
  - `1 ‚Üí Enrolled`  
  - `2 ‚Üí Graduate`  
- Este preprocesamiento asegura que los algoritmos de Machine Learning puedan trabajar con los datos sin problemas de codificaci√≥n.  




#### üìä Paso 4: Implementar Validaci√≥n Cruzada
```python
# === VALIDACI√ìN CRUZADA PARA ESTABILIDAD ===

print("üî¨ VALIDACI√ìN CRUZADA: ¬øQu√© tan estable es nuestro modelo?")

# 1. Crear pipeline robusto para usar en CV
pipeline_robust =  Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression(max_iter=1000, random_state=42))
])

print("Pipeline creado para validaci√≥n cruzada")

# 2. Crear KFold b√°sico
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# 3. Evaluar con KFold usando cross_val_score
scores_kfold = cross_val_score(
    pipeline_robust, X_features, y_target, cv=kfold, scoring='accuracy'
)

print(f"\nKFOLD RESULTS:")
print(f"   Scores individuales: {scores_kfold}")
print(f"   Media: {scores_kfold.mean():.4f}")
print(f"   Desviaci√≥n est√°ndar: {scores_kfold.std():.4f}")
print(f"   Resultado: {scores_kfold.mean():.4f} ¬± {scores_kfold.std():.4f}")

# 4. Crear StratifiedKFold (mantiene proporci√≥n de clases)
stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 5. Evaluar con StratifiedKFold
scores_stratified = cross_val_score(
    pipeline_robust, X_features, y_target, cv=stratified_kfold, scoring='accuracy'
)

print(f"\nSTRATIFIED KFOLD RESULTS:")
print(f"   Scores individuales: {scores_stratified}")
print(f"   Media: {scores_stratified.mean():.4f}")
print(f"   Desviaci√≥n est√°ndar: {scores_stratified.std():.4f}")
print(f"   Resultado: {scores_stratified.mean():.4f} ¬± {scores_stratified.std():.4f}")

# 6. Comparar estabilidad (menor desviaci√≥n = m√°s estable)
print(f"\nCOMPARACI√ìN DE ESTABILIDAD:")
if scores_stratified.std() < scores_kfold.std():
    print("   StratifiedKFold es M√ÅS ESTABLE (menor variabilidad)")
    mejor_cv = "StratifiedKFold"
else:
    print("   KFold es M√ÅS ESTABLE (menor variabilidad)")
    mejor_cv = "KFold"

print(f"   Recomendaci√≥n: Usar {mejor_cv} para este dataset")

# 7. Visualizar la distribuci√≥n de scores
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.boxplot([scores_kfold, scores_stratified], labels=['KFold', 'StratifiedKFold'])
plt.title('Distribuci√≥n de Scores - Validaci√≥n Cruzada')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3)
plt.show()
```
#### Salida:
```text
üî¨ VALIDACI√ìN CRUZADA: ¬øQu√© tan estable es nuestro modelo?
Pipeline creado para validaci√≥n cruzada

KFOLD RESULTS:
   Scores individuales: [0.75254237 0.76610169 0.76836158 0.77740113 0.78054299]
   Media: 0.7690
   Desviaci√≥n est√°ndar: 0.0098
   Resultado: 0.7690 ¬± 0.0098

STRATIFIED KFOLD RESULTS:
   Scores individuales: [0.76836158 0.76836158 0.76271186 0.75480226 0.75452489]
   Media: 0.7618
   Desviaci√≥n est√°ndar: 0.0061
   Resultado: 0.7618 ¬± 0.0061

COMPARACI√ìN DE ESTABILIDAD:
   StratifiedKFold es M√ÅS ESTABLE (menor variabilidad)
   Recomendaci√≥n: Usar StratifiedKFold para este dataset
/tmp/ipython-input-1194036424.py:55: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.
  plt.boxplot([scores_kfold, scores_stratified], labels=['KFold', 'StratifiedKFold'])
  ```

![02VAL](../assets/02VAL.png)


 üìå Interpretaci√≥n

- Se evalu√≥ un **pipeline** (escalado + regresi√≥n log√≠stica) con dos esquemas de validaci√≥n:
  - **KFold (5 folds)** ‚Üí **accuracy medio 0.7690** con **desv. std. 0.0098**.
  - **StratifiedKFold (5 folds)** ‚Üí **accuracy medio 0.7618** con **desv. std. 0.0061**.
- **Estabilidad**: StratifiedKFold muestra **menor variabilidad** entre folds (mejor estabilidad), aunque KFold obtuvo un promedio ligeramente mayor.
- En problemas de **clasificaci√≥n multiclase** (y potencial desbalance), **StratifiedKFold** es preferible porque **mantiene la proporci√≥n de clases** en cada fold.
- La comparaci√≥n sugiere:
  - Si priorizas **estabilidad y representatividad de clases**, usa **StratifiedKFold**.
  - Si solo miras el **promedio de accuracy**, KFold fue marginalmente superior en esta corrida.
-------------
üí≠ **¬øCu√°l m√©todo mantiene la proporci√≥n de clases en cada fold?**  
El m√©todo **StratifiedKFold** mantiene la **misma proporci√≥n de clases** en cada fold, lo que lo hace m√°s adecuado en problemas de clasificaci√≥n desbalanceada.

---

üìä **¬øCu√°l par√°metro especifica el tipo de validaci√≥n cruzada en `cross_val_score`?**  
El par√°metro es **`cv`**, donde puedes pasar:  
- Un entero ‚Üí n√∫mero de folds (ej. `cv=5` usa KFold simple).  
- Un objeto de validaci√≥n ‚Üí como `KFold`, `StratifiedKFold`, `LeaveOneOut`, etc.  
Esto determina **c√≥mo se divide el dataset** durante la validaci√≥n cruzada.

```python
# Cargar dataset de estudiantes desde UCI
student_data = fetch_ucirepo(id=697)

# Preparar datos
X = student_data.data.features
y = student_data.data.targets

print("Dataset: Student Dropout and Academic Success")
print(f"Estudiantes: {X.shape[0]}, Caracter√≠sticas: {X.shape[1]}")
print(f"Objetivo: Predecir {len(y.columns)} variable(s)")

# Explorar variable objetivo
target_col = y.columns[0]  # Primera columna objetivo
y_series = y[target_col]
print(f"\nVariable objetivo: {target_col}")

# Mapear valores para mejor interpretaci√≥n
target_mapping = {0: 'Dropout', 1: 'Enrolled', 2: 'Graduate'}
y_mapped = y_series.map(target_mapping)

# Distribuci√≥n de clases
print("\nDistribuci√≥n de resultados acad√©micos:")
value_counts = y_mapped.value_counts()
for outcome, count in value_counts.items():
    percentage = (count / len(y_mapped)) * 100
    print(f"  {outcome}: {count} estudiantes ({percentage:.1f}%)")

# Ver algunas caracter√≠sticas
print(f"\nPrimeras caracter√≠sticas:")
print(X.columns.tolist()[:10], "...")

# Estad√≠sticas b√°sicas
print(f"\nAge at enrollment:")
if 'Age at enrollment' in X.columns:
    age_col = X['Age at enrollment']
    print(f"  Promedio: {age_col.mean():.1f} a√±os")
    print(f"  Rango: {age_col.min():.0f}-{age_col.max():.0f} a√±os")
```
### üèÜ Parte 3: Comparaci√≥n de Modelos - ¬°El Torneo!
#### ü•ä Paso 5: Competencia de M√∫ltiples Modelos
```python
# === COMPETENCIA DE MODELOS ===

print("üèÜ TORNEO: ¬øCu√°l modelo funciona mejor para diagn√≥stico m√©dico?")

# 1. Definir candidatos (diferentes algoritmos)
models = {
    'Logistic Regression': Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(max_iter=1000, random_state=42))
    ]),

    # 2. Ridge Classifier (regresi√≥n log√≠stica con regularizaci√≥n L2)
    'Ridge Classifier': Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', RidgeClassifier(alpha=1.0, random_state=42))
    ]),

    # 3. Random Forest (ensemble, no necesita escalado)
    'Random Forest': Pipeline([
        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
    ])
}

print(f"Modelos en competencia: {list(models.keys())}")

# 4. Evaluar cada modelo con validaci√≥n cruzada
print(f"\nEVALUANDO MODELOS CON 5-FOLD CV...")

results = {}
for name, model in models.items():
    print(f"   Evaluando {name}...")

    # Usar StratifiedKFold para mantener balance de clases
    scores = cross_val_score(
        model, X_features, y_target,
        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
        scoring='accuracy'
    )

    results[name] = scores

    print(f"   {name}: {scores.mean():.4f} ¬± {scores.std():.4f}")
    print(f"      Scores: {[f'{s:.3f}' for s in scores]}")

# 5. Encontrar el mejor modelo
print(f"\nRESULTADOS FINALES:")

# Encontrar modelo con mayor accuracy promedio
best_mean_score = 0
best_model_name = ""

for name, scores in results.items():
    if scores.mean() > best_mean_score:
        best_mean_score = scores.mean()
        best_model_name = name

print(f"GANADOR: {best_model_name}")
print(f"Score: {best_mean_score:.4f}")

# 6. An√°lisis detallado de estabilidad
print(f"\nAN√ÅLISIS DE ESTABILIDAD:")
for name, scores in results.items():
    stability = scores.std()

    if stability < 0.02:
        status = "MUY ESTABLE"
    elif stability < 0.05:
        status = "ESTABLE"
    else:
        status = "INESTABLE"

    print(f"   {name}: {status} (std: {stability:.4f})")

# 7. Visualizaci√≥n comparativa
plt.figure(figsize=(12, 6))

# Boxplot de distribuci√≥n de scores
plt.subplot(1, 2, 1)
plt.boxplot([results[name] for name in models.keys()],
           labels=[name.split()[0] for name in models.keys()])
plt.title('Distribuci√≥n de Accuracy por Modelo')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3)

# Barplot de medias con error bars
plt.subplot(1, 2, 2)
names = list(models.keys())
means = [results[name].mean() for name in names]
stds = [results[name].std() for name in names]

plt.bar(range(len(names)), means, yerr=stds, capsize=5)
plt.xticks(range(len(names)), [name.split()[0] for name in names])
plt.title('Accuracy Promedio ¬± Desviaci√≥n Est√°ndar')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```
#### Salida:
```text
üèÜ TORNEO: ¬øCu√°l modelo funciona mejor para diagn√≥stico m√©dico?
Modelos en competencia: ['Logistic Regression', 'Ridge Classifier', 'Random Forest']

EVALUANDO MODELOS CON 5-FOLD CV...
   Evaluando Logistic Regression...
   Logistic Regression: 0.7618 ¬± 0.0061
      Scores: ['0.768', '0.768', '0.763', '0.755', '0.755']
   Evaluando Ridge Classifier...
   Ridge Classifier: 0.7509 ¬± 0.0032
      Scores: ['0.755', '0.746', '0.754', '0.749', '0.751']
   Evaluando Random Forest...
   Random Forest: 0.7658 ¬± 0.0064
      Scores: ['0.775', '0.764', '0.771', '0.763', '0.757']

RESULTADOS FINALES:
GANADOR: Random Forest
Score: 0.7658

AN√ÅLISIS DE ESTABILIDAD:
   Logistic Regression: MUY ESTABLE (std: 0.0061)
   Ridge Classifier: MUY ESTABLE (std: 0.0032)
   Random Forest: MUY ESTABLE (std: 0.0064)
/tmp/ipython-input-1230983371.py:79: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.
  plt.boxplot([results[name] for name in models.keys()],
```
![02VAL](../assets/002VAL.png)

 Interpretaci√≥n

- **Modelos evaluados:** Logistic Regression, Ridge Classifier y Random Forest, todos validados con **StratifiedKFold (5 folds)** dentro de un **Pipeline** (evita data leakage).
- **Desempe√±o (accuracy medio ¬± std):**
  - Logistic Regression: **0.7618 ¬± 0.0061**
  - Ridge Classifier: **0.7509 ¬± 0.0032**
  - Random Forest: **0.7658 ¬± 0.0064**  ‚Üê **mejor promedio**
- **Ganador:** *Random Forest* por un margen peque√±o sobre Logistic Regression (~0.4 puntos porcentuales).
- **Estabilidad:** los tres modelos muestran **muy baja variabilidad** entre folds (std ‚âà 0.003‚Äì0.006), por lo que los resultados son **consistentes**.
- **Lectura pr√°ctica:** la diferencia entre Random Forest y Logistic Regression es **modesta**; con ajuste de hiperpar√°metros (p. ej., `n_estimators`, `max_depth` en RF; `C`, `penalty` en LR) el orden podr√≠a cambiar.

üí≠ **Random Forest no necesita escalado, ¬øpor qu√© no incluir StandardScaler?**  
Porque Random Forest y otros modelos basados en √°rboles (como Decision Trees o Gradient Boosting) **no dependen de la magnitud de las variables**. Dividen el espacio en base a umbrales, no a distancias.  
üëâ Incluir `StandardScaler` no da√±a el modelo, pero es **innecesario** y solo a√±ade complejidad extra al pipeline.  

---

üìä **¬øCu√°l m√©trica es mejor para clasificaci√≥n: 'accuracy', 'precision', o 'f1'?**  
Depende del problema:  
- **Accuracy**: √∫til si las clases est√°n **balanceadas**.  
- **Precision**: prioriza reducir falsos positivos (ejemplo: diagn√≥stico de enfermedad grave).  
- **Recall**: prioriza reducir falsos negativos (ejemplo: detectar fraudes o c√°ncer).  
- **F1-score**: balance entre precisi√≥n y recall; mejor cuando hay **desbalance de clases** y se necesitan ambas.  

üëâ En este dataset multiclase con cierto desbalance, **macro-F1** o **balanced accuracy** son m√°s informativas que accuracy.  

---

üîç **Los m√©todos `.mean()` y `.std()` funcionan con arrays de numpy**  
Correcto ‚úÖ.  
- `scores.mean()` calcula el **promedio** de los resultados de validaci√≥n cruzada.  
- `scores.std()` mide la **variabilidad (desviaci√≥n est√°ndar)** entre los folds.  
Esto permite cuantificar no solo el rendimiento promedio, sino tambi√©n la **estabilidad** del modelo.  

### üìö BONUS: ¬øQu√© significan las m√©tricas de validaci√≥n?

- **Cross-Validation**: t√©cnica que divide los datos en *k folds* para entrenar y evaluar el modelo m√∫ltiples veces, reduciendo el riesgo de sobreajuste y ofreciendo una mejor estimaci√≥n del desempe√±o en datos nuevos.  

- **Accuracy promedio**: la media de los resultados de accuracy en los distintos folds; refleja el rendimiento esperado del modelo en datos no vistos.  

- **Desviaci√≥n est√°ndar**: mide la variabilidad del accuracy entre folds.  
  - **Baja desviaci√≥n** ‚Üí modelo estable y consistente.  
  - **Alta desviaci√≥n** ‚Üí modelo sensible a c√≥mo se dividen los datos.  

- **StratifiedKFold**: variante de validaci√≥n cruzada que asegura que la **proporci√≥n de clases** se mantenga en cada fold, siendo especialmente √∫til cuando el dataset est√° **desbalanceado**.  

üí° **PISTAS**

üìä **Un modelo estable tiene baja o alta variabilidad?**  
Un modelo estable tiene **baja variabilidad** (desviaci√≥n est√°ndar peque√±a entre folds).  

ü©∫ **En medicina, ¬øprefieres un modelo consistente o uno que var√≠a mucho?**  
En contextos m√©dicos es preferible un modelo **consistente y estable**, ya que las decisiones cl√≠nicas deben ser confiables y no depender demasiado de c√≥mo se dividan los datos de entrenamiento.  

### üöÄ BONUS: Optimizaci√≥n de Hiperpar√°metros
#### Paso 6: GridSearchCV vs RandomizedSearchCV

```python
from sklearn.model_selection import GridSearchCV , RandomizedSearchCV

# Seleccionar el mejor modelo de la competencia anterior
best_model_base = models[best_model_name]

print(f"Optimizando hiperpar√°metros para: {best_model_name}")

# Definir espacio de b√∫squeda de hiperpar√°metros
if 'Random Forest' in best_model_name:
    param_grid = {
        'classifier__n_estimators': [50, 100, 200],
        'classifier__max_depth': [None, 10, 20, 30],
        'classifier__min_samples_split': [2, 5, 10]
    }
elif 'Logistic' in best_model_name:
    param_grid = {
        'classifier__C': [0.1, 1, 10, 100],
        'classifier__max_iter': [1000, 2000]
    }
else:  # Ridge
    param_grid = {
        'classifier__alpha': [0.1, 1, 10, 100]
    }

# M√âTODO 1: GridSearchCV (b√∫squeda exhaustiva)
print("\nM√©todo 1: GridSearchCV (b√∫squeda exhaustiva)")
grid_search = GridSearchCV(
    best_model_base,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_features, y_target)

print(f"Mejores par√°metros (Grid): {grid_search.best_params_}")
print(f"Mejor score (Grid): {grid_search.best_score_:.4f}")

# M√âTODO 2: RandomizedSearchCV (b√∫squeda aleatoria, m√°s eficiente)
print("\nM√©todo 2: RandomizedSearchCV (b√∫squeda aleatoria)")
random_search = RandomizedSearchCV(
    best_model_base,
    param_grid,
    n_iter=20,  # Solo 20 combinaciones aleatorias
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

random_search.fit(X_features, y_target)

print(f"Mejores par√°metros (Random): {random_search.best_params_}")
print(f"Mejor score (Random): {random_search.best_score_:.4f}")

# Comparar eficiencia
print(f"\nComparaci√≥n de eficiencia:")
print(f"GridSearch prob√≥: {len(grid_search.cv_results_['params'])} combinaciones")
print(f"RandomSearch prob√≥: {len(random_search.cv_results_['params'])} combinaciones")

# Evaluar modelo final optimizado
final_model = grid_search.best_estimator_
final_scores = cross_val_score(final_model, X_features, y_target, cv=5)
print(f"\nModelo final optimizado: {final_scores.mean():.4f} ¬± {final_scores.std():.4f}")
```

#### Salida:

```text
Optimizando hiperpar√°metros para: Random Forest

M√©todo 1: GridSearchCV (b√∫squeda exhaustiva)
Fitting 5 folds for each of 36 candidates, totalling 180 fits
Mejores par√°metros (Grid): {'classifier__max_depth': None, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 100}
Mejor score (Grid): 0.7783

M√©todo 2: RandomizedSearchCV (b√∫squeda aleatoria)
Fitting 5 folds for each of 20 candidates, totalling 100 fits
Mejores par√°metros (Random): {'classifier__n_estimators': 100, 'classifier__min_samples_split': 5, 'classifier__max_depth': 30}
Mejor score (Random): 0.7783

Comparaci√≥n de eficiencia:
GridSearch prob√≥: 36 combinaciones
RandomSearch prob√≥: 20 combinaciones

Modelo final optimizado: 0.7783 ¬± 0.0067
```

üìå Interpretaci√≥n (GridSearchCV vs RandomizedSearchCV)

- **Modelo objetivo:** Random Forest (ganador previo del torneo).
- **Resultados:**
  - **GridSearchCV** (b√∫squeda exhaustiva): prob√≥ **36** combinaciones ‚Üí **best score = 0.7783** con `n_estimators=100`, `min_samples_split=5`, `max_depth=None`.
  - **RandomizedSearchCV** (b√∫squeda aleatoria): prob√≥ **20** combinaciones ‚Üí **best score = 0.7783** con `n_estimators=100`, `min_samples_split=5`, `max_depth=30`.
  - **Eficiencia:** Randomized alcanz√≥ el **mismo mejor score** con **44% menos** de combinaciones evaluadas.
  - **Validaci√≥n final:** modelo optimizado ‚Üí **0.7783 ¬± 0.0067** (5-fold CV), desempe√±o **consistente**.

- **Lectura pr√°ctica:**
  - Cuando el **espacio de hiperpar√°metros es peque√±o**, **GridSearchCV** es viable y garantiza cubrir todo el grid.
  - Cuando el **espacio es grande** o hay **poco presupuesto** de c√≥mputo/tiempo, **RandomizedSearchCV** suele ser **m√°s eficiente** y encuentra configuraciones competitivas.
  - Las dos configuraciones ganadoras son **muy cercanas** (profundidad ilimitada vs 30). La diferencia pr√°ctica es m√≠nima dentro de la variabilidad de CV.
  - Mantener el **Pipeline** en la b√∫squeda evita *data leakage* (aunque RF no necesita escalado, el pipeline est√°ndar es correcto y homog√©neo para comparar modelos).

### üìù Gu√≠a de decisi√≥n

- **GridSearchCV** cuando tienes **pocos hiperpar√°metros** y **suficiente tiempo de c√≥mputo**.  

- **RandomizedSearchCV** cuando tienes **muchos hiperpar√°metros** o **tiempo limitado**.  

- **Pipeline + SearchCV** siempre previene **data leakage** autom√°ticamente.  

- **cross_val_score** en el resultado final valida que la optimizaci√≥n no caus√≥ **sobreajuste (overfitting)**.  

### üîç BONUS 2: Explicabilidad del Modelo

#### Paso 7: ¬øPor qu√© el modelo toma esas decisiones?
```python
# Usar Random Forest para explicabilidad (si no gan√≥, crearlo)
if 'Random Forest' not in best_model_name:
    # Crear Random Forest espec√≠fico para explicabilidad
    # Random Forest no necesita escalado, as√≠ que lo omitimos para simplicidad
    rf_model = Pipeline([
        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
    ])
    rf_model.fit(X_features, y_target)
    print("Creado Random Forest espec√≠fico para an√°lisis de explicabilidad")
else:
    rf_model = final_model
    print("Usando el modelo ganador para explicabilidad")

# Verificar estructura del pipeline
print(f"Componentes del pipeline: {list(rf_model.named_steps.keys())}")

# 1. FEATURE IMPORTANCE - ¬øQu√© caracter√≠sticas son m√°s importantes?
feature_names = X_features.columns
importances = rf_model.named_steps['classifier'].feature_importances_

# Crear DataFrame para mejor visualizaci√≥n
feature_importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': importances
}).sort_values('importance', ascending=False)

print("\nTOP 10 CARACTER√çSTICAS M√ÅS IMPORTANTES:")
for i, row in feature_importance_df.head(10).iterrows():
    print(f"{row['feature']}: {row['importance']:.4f}")

# Visualizar importancia de caracter√≠sticas
plt.figure(figsize=(10, 8))
top_features = feature_importance_df.head(15)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('Importancia')
plt.title('Top 15 Caracter√≠sticas M√°s Importantes para Predecir √âxito Estudiantil')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# 2. AN√ÅLISIS POR CATEGOR√çAS - Agrupar caracter√≠sticas relacionadas
academic_features = [col for col in feature_names if any(word in col.lower()
                    for word in ['grade', 'units', 'curricular', 'semester'])]
demographic_features = [col for col in feature_names if any(word in col.lower()
                       for word in ['age', 'gender', 'nationality', 'marital'])]
economic_features = [col for col in feature_names if any(word in col.lower()
                    for word in ['scholarship', 'debt', 'fee', 'tuition'])]

def calculate_category_importance(features, importance_df):
    if not features:
        return 0
    category_importance = importance_df[importance_df['feature'].isin(features)]['importance'].sum()
    return category_importance

academic_importance = calculate_category_importance(academic_features, feature_importance_df)
demographic_importance = calculate_category_importance(demographic_features, feature_importance_df)
economic_importance = calculate_category_importance(economic_features, feature_importance_df)

print(f"\nIMPORTANCIA POR CATEGOR√çAS:")
print(f"Factores acad√©micos: {academic_importance:.4f}")
print(f"Factores demogr√°ficos: {demographic_importance:.4f}")
print(f"Factores econ√≥micos: {economic_importance:.4f}")

# 3. INTERPRETACI√ìN PR√ÅCTICA - ¬øQu√© significa esto?
print(f"\nINTERPRETACI√ìN PARA INTERVENCIONES:")
print(f"La caracter√≠stica m√°s importante es: {feature_importance_df.iloc[0]['feature']}")
print(f"Esto sugiere que para reducir abandono estudiantil debemos enfocarnos en:")

# Generar recomendaciones basadas en las top features
top_3_features = feature_importance_df.head(3)['feature'].tolist()
for i, feature in enumerate(top_3_features, 1):
    print(f"{i}. Monitorear y mejorar: {feature}")

# 4. PREDICCI√ìN INDIVIDUAL - ¬øPor qu√© un estudiante espec√≠fico est√° en riesgo?
print(f"\nAN√ÅLISIS DE ESTUDIANTE INDIVIDUAL (ejemplo):")
student_idx = 0
student_data = X_features.iloc[student_idx:student_idx+1]
prediction = rf_model.predict(student_data)[0]
prediction_proba = rf_model.predict_proba(student_data)[0]

# Definir mapeo localmente para esta secci√≥n
outcome_mapping = {0: 'Dropout', 1: 'Enrolled', 2: 'Graduate'}

# Manejar si prediction es string o n√∫mero
if isinstance(prediction, str):
    predicted_outcome = prediction
else:
    predicted_outcome = outcome_mapping[prediction]

print(f"Estudiante #{student_idx}:")
print(f"Predicci√≥n: {predicted_outcome}")
print(f"Probabilidades:")
for i, prob in enumerate(prediction_proba):
    outcome_name = outcome_mapping[i]
    print(f"  {outcome_name}: {prob:.3f}")

# Mostrar las caracter√≠sticas m√°s importantes de este estudiante
student_features = pd.DataFrame({
    'feature': feature_names,
    'value': student_data.iloc[0].values,
    'importance': importances
}).sort_values('importance', ascending=False)

print(f"\nTop 5 caracter√≠sticas que influyen en esta predicci√≥n:")
for i, row in student_features.head(5).iterrows():
    print(f"{row['feature']}: {row['value']:.2f} (importancia: {row['importance']:.4f})")

# 5. VISUALIZACI√ìN DE √ÅRBOLES INDIVIDUALES
print(f"\nVISUALIZACI√ìN DE √ÅRBOLES DEL RANDOM FOREST:")

# Instalar graphviz si no est√° disponible
try:
    from sklearn.tree import export_graphviz, plot_tree, export_text
    import matplotlib.pyplot as plt

    # Obtener algunos √°rboles del bosque
    forest = rf_model.named_steps['classifier']
    n_trees_to_show = min(3, len(forest.estimators_))

    print(f"Mostrando {n_trees_to_show} √°rboles de {len(forest.estimators_)} totales")

    # Visualizar √°rboles con plot_tree (m√°s simple)
    fig, axes = plt.subplots(1, n_trees_to_show, figsize=(25, 12))
    if n_trees_to_show == 1:
        axes = [axes]

    for i in range(n_trees_to_show):
        tree = forest.estimators_[i]

        # Limitar profundidad para que sea legible
        plot_tree(tree,
                 ax=axes[i],
                 feature_names=list(feature_names),  # Usar todos los nombres de caracter√≠sticas
                 class_names=list(outcome_mapping.values()),
                 filled=True,
                 max_depth=3,  # Limitar profundidad
                 fontsize=6)  # Fuente m√°s peque√±a para que quepa

        axes[i].set_title(f'√Årbol {i+1} (profundidad m√°x: 3)', fontsize=12)

    plt.tight_layout()
    plt.show()

    # Informaci√≥n sobre la estructura de los √°rboles
    print(f"\nESTAD√çSTICAS DE LOS √ÅRBOLES:")
    depths = [tree.get_depth() for tree in forest.estimators_[:5]]
    n_nodes = [tree.tree_.node_count for tree in forest.estimators_[:5]]

    print(f"Profundidad promedio (primeros 5 √°rboles): {sum(depths)/len(depths):.1f}")
    print(f"N√∫mero promedio de nodos (primeros 5): {sum(n_nodes)/len(n_nodes):.1f}")

    # Mostrar un √°rbol muy simple por texto
    print(f"\nEJEMPLO DE REGLAS DE DECISI√ìN (√Årbol 1, simplificado):")
    tree_rules = export_text(forest.estimators_[0],
                           feature_names=list(feature_names),
                           max_depth=2)
    print(tree_rules[:500] + "..." if len(tree_rules) > 500 else tree_rules)

except ImportError:
    print("Para visualizar √°rboles, instala: pip install graphviz")
    print("Alternativamente, mostramos la estructura del bosque:")

    forest = rf_model.named_steps['classifier']
    print(f"Random Forest contiene {len(forest.estimators_)} √°rboles")
    print(f"Cada √°rbol fue entrenado con {forest.max_features_} caracter√≠sticas aleatorias")

    # Estad√≠sticas b√°sicas sin visualizaci√≥n
    if len(forest.estimators_) > 0:
        depths = [tree.get_depth() for tree in forest.estimators_[:5]]
        print(f"Profundidad promedio: {sum(depths)/len(depths):.1f}")

# 6. DIVERSIDAD DEL BOSQUE
print(f"\nDIVERSIDAD EN EL RANDOM FOREST:")
print("El poder del Random Forest viene de la diversidad de sus √°rboles:")
print("- Cada √°rbol ve una muestra diferente de datos (bootstrap)")
print("- Cada split considera solo un subconjunto aleatorio de caracter√≠sticas")
print("- La predicci√≥n final es el voto mayoritario de todos los √°rboles")

# Mostrar diferencias en predicciones individuales
student_sample = X_features.iloc[0:1]
individual_predictions = []

# Preparar datos dependiendo de si el modelo tiene scaler o no
if 'scaler' in rf_model.named_steps:
    # Modelo con scaler
    scaled_sample = rf_model.named_steps['scaler'].transform(student_sample)
    print("Usando datos escalados para √°rboles individuales")
else:
    # Modelo sin scaler (ej: Random Forest sin preprocesamiento)
    scaled_sample = student_sample.values
    print("Usando datos sin escalar para √°rboles individuales")

for i, tree in enumerate(forest.estimators_[:5]):
    tree_pred = tree.predict(scaled_sample)[0]
    individual_predictions.append(tree_pred)

print(f"\nPredicciones de √°rboles individuales para el Estudiante #0:")
for i, pred in enumerate(individual_predictions):
    pred_name = outcome_mapping[pred] if isinstance(pred, int) else pred
    print(f"  √Årbol {i+1}: {pred_name}")

final_pred = max(set(individual_predictions), key=individual_predictions.count)
final_pred_name = outcome_mapping[final_pred] if isinstance(final_pred, int) else final_pred
print(f"Predicci√≥n final (voto mayoritario): {final_pred_name}")
```

#### Salida:
```text
Usando el modelo ganador para explicabilidad
Componentes del pipeline: ['classifier']

TOP 10 CARACTER√çSTICAS M√ÅS IMPORTANTES:
Curricular units 2nd sem (approved): 0.1516
Curricular units 2nd sem (grade): 0.1193
Curricular units 1st sem (approved): 0.0987
Curricular units 1st sem (grade): 0.0589
Tuition fees up to date: 0.0466
Curricular units 2nd sem (evaluations): 0.0419
Admission grade: 0.0385
Age at enrollment: 0.0372
Curricular units 1st sem (evaluations): 0.0349
Previous qualification (grade): 0.0343
```
![0002VAL](../assets/0002VAL.png)
```text
IMPORTANCIA POR CATEGOR√çAS:
Factores acad√©micos: 0.6443
Factores demogr√°ficos: 0.0499
Factores econ√≥micos: 0.0769

INTERPRETACI√ìN PARA INTERVENCIONES:
La caracter√≠stica m√°s importante es: Curricular units 2nd sem (approved)
Esto sugiere que para reducir abandono estudiantil debemos enfocarnos en:
1. Monitorear y mejorar: Curricular units 2nd sem (approved)
2. Monitorear y mejorar: Curricular units 2nd sem (grade)
3. Monitorear y mejorar: Curricular units 1st sem (approved)

AN√ÅLISIS DE ESTUDIANTE INDIVIDUAL (ejemplo):
Estudiante #0:
Predicci√≥n: Dropout
Probabilidades:
  Dropout: 0.737
  Enrolled: 0.082
  Graduate: 0.181

Top 5 caracter√≠sticas que influyen en esta predicci√≥n:
Curricular units 2nd sem (approved): 0.00 (importancia: 0.1516)
Curricular units 2nd sem (grade): 0.00 (importancia: 0.1193)
Curricular units 1st sem (approved): 0.00 (importancia: 0.0987)
Curricular units 1st sem (grade): 0.00 (importancia: 0.0589)
Tuition fees up to date: 1.00 (importancia: 0.0466)

VISUALIZACI√ìN DE √ÅRBOLES DEL RANDOM FOREST:
Mostrando 3 √°rboles de 100 totales
```
![0002VAL](../assets/00002VAL.png)
```text
ESTAD√çSTICAS DE LOS √ÅRBOLES:
Profundidad promedio (primeros 5 √°rboles): 21.2
N√∫mero promedio de nodos (primeros 5): 1139.0

EJEMPLO DE REGLAS DE DECISI√ìN (√Årbol 1, simplificado):
|--- Curricular units 2nd sem (approved) <= 3.50
|   |--- Curricular units 2nd sem (evaluations) <= 7.50
|   |   |--- Curricular units 1st sem (enrolled) <= 0.50
|   |   |   |--- truncated branch of depth 10
|   |   |--- Curricular units 1st sem (enrolled) >  0.50
|   |   |   |--- truncated branch of depth 10
|   |--- Curricular units 2nd sem (evaluations) >  7.50
|   |   |--- Age at enrollment <= 25.50
|   |   |   |--- truncated branch of depth 16
|   |   |--- Age at enrollment >  25.50
|   |  ...

DIVERSIDAD EN EL RANDOM FOREST:
El poder del Random Forest viene de la diversidad de sus √°rboles:
- Cada √°rbol ve una muestra diferente de datos (bootstrap)
- Cada split considera solo un subconjunto aleatorio de caracter√≠sticas
- La predicci√≥n final es el voto mayoritario de todos los √°rboles
Usando datos sin escalar para √°rboles individuales

Predicciones de √°rboles individuales para el Estudiante #0:
  √Årbol 1: 2.0
  √Årbol 2: 0.0
  √Årbol 3: 0.0
  √Årbol 4: 0.0
  √Årbol 5: 0.0
Predicci√≥n final (voto mayoritario): 0.0
```
```text
üìå Interpretaci√≥n 

- **Importancias globales (Random Forest):** el modelo se apoya principalmente en el **rendimiento acad√©mico reciente**:
  - *Curricular units 2nd sem (approved / grade)* y *1st sem (approved / grade)* son las se√±ales dominantes.
  - **Implicaci√≥n:** la trayectoria de notas y aprobaciones por semestre es el predictor m√°s fuerte de *Dropout / Enrolled / Graduate*.

- **Se√±ales administrativas y de perfil:**
  - *Tuition fees up to date* aporta informaci√≥n √∫til (riesgo econ√≥mico/administrativo).
  - *Admission grade*, *Previous qualification (grade)* y *Age at enrollment* tambi√©n contribuyen (bagaje previo y momento de vida).

- **Importancia por categor√≠as (suma de importancias):**
  - **Acad√©micos:** 0.6443 ‚Üí **mayor peso**.
  - **Econ√≥micos:** 0.0769 ‚Üí riesgo operativo/financiero.
  - **Demogr√°ficos:** 0.0499 ‚Üí efecto moderado.
```
#### ü§î ¬øPor qu√© es importante la explicabilidad?

- **Confianza:** Los educadores necesitan **entender claramente** por qu√© el modelo predice abandono.  
- **Intervenciones:** Knowing las caracter√≠sticas importantes permite crear **acciones de apoyo** espec√≠ficas.  
- **Bias detection:** La explicabilidad ayuda a detectar **sesgos** en el modelo.  
- **Regulaciones:** Muchos contextos requieren modelos **transparentes y auditables** por ley.  
- **Mejora continua:** Entender el modelo ayuda a **ajustar y optimizar** futuras versiones.  
-----------------
### üéØ Parte 4: Preguntas de Reflexi√≥n

**1. ¬øQu√© es *data leakage* y por qu√© es peligroso?**  
El *data leakage* ocurre cuando el modelo accede a informaci√≥n del conjunto de prueba o del futuro durante el entrenamiento.  
Es peligroso porque hace que el modelo tenga un rendimiento artificialmente alto en validaci√≥n, pero falle al enfrentarse a datos reales.  

---

**2. ¬øCu√°ndo usar KFold vs StratifiedKFold?**  
- **KFold:** √∫til cuando las clases est√°n balanceadas o el problema es de regresi√≥n.  
- **StratifiedKFold:** preferible en clasificaci√≥n, especialmente cuando una clase tiene pocas muestras, porque mantiene la proporci√≥n de clases en cada fold.  

---

**3. ¬øC√≥mo interpretar "95.2% ¬± 2.1%" en cross-validation?**  
- **95.2%:** es el accuracy promedio esperado del modelo en datos nuevos.  
- **¬± 2.1%:** indica la variabilidad entre folds; mientras menor sea, m√°s estable y confiable es el modelo.  

---

**4. ¬øPor qu√© Random Forest no necesita StandardScaler?**  
Porque los √°rboles de decisi√≥n (y sus ensembles como Random Forest) dividen el espacio de datos en base a umbrales (ej. `edad > 20`).  
Estas divisiones no dependen de la escala de las variables, a diferencia de modelos basados en distancia como KNN o SVM.  

---

**5. En diagn√≥stico m√©dico, ¬øprefieres un modelo con 98% accuracy pero inestable, o 95% accuracy pero muy estable?**  
Es mejor un modelo **m√°s estable (95% pero consistente)**.  
En medicina la confiabilidad es clave: un modelo inestable puede dar resultados impredecibles seg√∫n la partici√≥n de los datos, lo que ser√≠a riesgoso en decisiones cl√≠nicas.  

---
## Evidencias
- Notebook en Google Colab con el desarrollo completo:  
  [![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1IUB3APVkyk09Prkas42hy9PW92oNNS_8?usp=sharing)


## Reflexi√≥n


En esta pr√°ctica aprend√≠ la importancia de usar pipelines para prevenir *data leakage* y asegurar un flujo de trabajo reproducible.  
Tambi√©n reforc√© c√≥mo la validaci√≥n cruzada (especialmente con **StratifiedKFold**) permite evaluar la estabilidad de los modelos en datasets desbalanceados.  

Entend√≠ que la comparaci√≥n entre varios algoritmos no solo debe basarse en la m√©trica promedio, sino tambi√©n en la **consistencia (desviaci√≥n est√°ndar)**.  
La optimizaci√≥n de hiperpar√°metros con GridSearchCV y RandomizedSearchCV me permiti√≥ ver las diferencias entre una b√∫squeda exhaustiva y otra m√°s eficiente.  

Finalmente, la parte de explicabilidad con **Random Forest** mostr√≥ c√≥mo identificar qu√© variables son m√°s influyentes y c√≥mo esto se traduce en decisiones pr√°cticas para reducir el abandono estudiantil.  

Si volviera a repetir esta pr√°ctica, mejorar√≠a la **visualizaci√≥n de resultados** (gr√°ficas comparativas, reportes m√°s claros) y explorar√≠a m√°s m√©tricas adem√°s del *accuracy*, como el **F1-score o balanced accuracy**, para tener una visi√≥n m√°s completa.  
El pr√≥ximo paso ser√≠a aplicar estas t√©cnicas en **otros datasets reales** y profundizar en m√©todos de interpretabilidad m√°s avanzados como SHAP o LIME.
